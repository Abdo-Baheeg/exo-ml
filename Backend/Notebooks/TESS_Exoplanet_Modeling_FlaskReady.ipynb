{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5696184f",
   "metadata": {},
   "source": [
    "# TESS Exoplanet Modeling (Flask-ready)\n",
    "\n",
    "**Notebook:** `TESS_Exoplanet_Modeling_FlaskReady.ipynb`\n",
    "\n",
    "**Purpose:** Full ML workflow for TESS dataset — feature selection, outlier handling, training (RandomForest, XGBoost, LogisticRegression),\n",
    "evaluation, extraction of top-5 features, and saving artifacts (models, plots, JSON) for a Flask web backend.\n",
    "\n",
    "**Notes:**\n",
    "- Dataset expected at `/mnt/data/TESS.csv`.\n",
    "- Update mapping or column names in the Feature Engineering step if your TESS file uses different headers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963bbce",
   "metadata": {},
   "source": [
    "## Step 1 — Imports & Global Config\n",
    "Import libraries, suppress warnings, and define directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7426077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories prepared: static\\models static\\plots static\\results\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, roc_curve)\n",
    "import joblib\n",
    "\n",
    "# optional: XGBoost may not be installed in every environment; wrapped import\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# Directories for Flask/static artifacts\n",
    "BASE_MODEL_DIR = r'static\\models'\n",
    "PLOTS_DIR = r'static\\plots'\n",
    "RESULTS_DIR = r'static\\results'\n",
    "os.makedirs(BASE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "sns.set(style='whitegrid')\n",
    "print(\"Directories prepared:\", BASE_MODEL_DIR, PLOTS_DIR, RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2decd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TESS dataset, shape: (7703, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>toi</th>\n",
       "      <th>toipfx</th>\n",
       "      <th>tid</th>\n",
       "      <th>ctoi_alias</th>\n",
       "      <th>pl_pnum</th>\n",
       "      <th>tfopwg_disp</th>\n",
       "      <th>rastr</th>\n",
       "      <th>ra</th>\n",
       "      <th>decstr</th>\n",
       "      <th>...</th>\n",
       "      <th>pl_rade</th>\n",
       "      <th>pl_insol</th>\n",
       "      <th>pl_eqt</th>\n",
       "      <th>st_tmag</th>\n",
       "      <th>st_dist</th>\n",
       "      <th>st_teff</th>\n",
       "      <th>st_logg</th>\n",
       "      <th>st_rad</th>\n",
       "      <th>toi_created</th>\n",
       "      <th>rowupdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>50365310</td>\n",
       "      <td>5.036531e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h29m25.85s</td>\n",
       "      <td>112.357708</td>\n",
       "      <td>-12d41m45.46s</td>\n",
       "      <td>...</td>\n",
       "      <td>5.818163</td>\n",
       "      <td>22601.94858</td>\n",
       "      <td>3127.204052</td>\n",
       "      <td>9.604000</td>\n",
       "      <td>485.735</td>\n",
       "      <td>10249.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>2.16986</td>\n",
       "      <td>24/07/2019 15:58</td>\n",
       "      <td>09/09/2024 10:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1001.01</td>\n",
       "      <td>1001</td>\n",
       "      <td>88863718</td>\n",
       "      <td>8.886372e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "      <td>08h10m19.31s</td>\n",
       "      <td>122.580465</td>\n",
       "      <td>-05d30m49.87s</td>\n",
       "      <td>...</td>\n",
       "      <td>11.215400</td>\n",
       "      <td>44464.50000</td>\n",
       "      <td>4045.000000</td>\n",
       "      <td>9.423440</td>\n",
       "      <td>295.862</td>\n",
       "      <td>7070.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.01000</td>\n",
       "      <td>24/07/2019 15:58</td>\n",
       "      <td>03/04/2023 14:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1002.01</td>\n",
       "      <td>1002</td>\n",
       "      <td>124709665</td>\n",
       "      <td>1.247097e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>FP</td>\n",
       "      <td>06h58m54.47s</td>\n",
       "      <td>104.726966</td>\n",
       "      <td>-10d34m49.64s</td>\n",
       "      <td>...</td>\n",
       "      <td>23.752900</td>\n",
       "      <td>2860.61000</td>\n",
       "      <td>2037.000000</td>\n",
       "      <td>9.299501</td>\n",
       "      <td>943.109</td>\n",
       "      <td>8924.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.73000</td>\n",
       "      <td>24/07/2019 15:58</td>\n",
       "      <td>11/07/2022 16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1003.01</td>\n",
       "      <td>1003</td>\n",
       "      <td>106997505</td>\n",
       "      <td>1.069975e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h22m14.39s</td>\n",
       "      <td>110.559945</td>\n",
       "      <td>-25d12m25.26s</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1177.36000</td>\n",
       "      <td>1631.000000</td>\n",
       "      <td>9.300300</td>\n",
       "      <td>7728.170</td>\n",
       "      <td>5388.5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24/07/2019 15:58</td>\n",
       "      <td>23/02/2022 10:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1004.01</td>\n",
       "      <td>1004</td>\n",
       "      <td>238597883</td>\n",
       "      <td>2.385979e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>FP</td>\n",
       "      <td>08h08m42.77s</td>\n",
       "      <td>122.178195</td>\n",
       "      <td>-48d48m10.12s</td>\n",
       "      <td>...</td>\n",
       "      <td>11.311300</td>\n",
       "      <td>54679.30000</td>\n",
       "      <td>4260.000000</td>\n",
       "      <td>9.135500</td>\n",
       "      <td>356.437</td>\n",
       "      <td>9219.0</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.15000</td>\n",
       "      <td>24/07/2019 15:58</td>\n",
       "      <td>09/09/2024 10:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rowid      toi  toipfx        tid    ctoi_alias  pl_pnum tfopwg_disp  \\\n",
       "0      1  1000.01    1000   50365310  5.036531e+07        1          FP   \n",
       "1      2  1001.01    1001   88863718  8.886372e+07        1          PC   \n",
       "2      3  1002.01    1002  124709665  1.247097e+08        1          FP   \n",
       "3      4  1003.01    1003  106997505  1.069975e+08        1          FP   \n",
       "4      5  1004.01    1004  238597883  2.385979e+08        1          FP   \n",
       "\n",
       "          rastr          ra         decstr  ...    pl_rade     pl_insol  \\\n",
       "0  07h29m25.85s  112.357708  -12d41m45.46s  ...   5.818163  22601.94858   \n",
       "1  08h10m19.31s  122.580465  -05d30m49.87s  ...  11.215400  44464.50000   \n",
       "2  06h58m54.47s  104.726966  -10d34m49.64s  ...  23.752900   2860.61000   \n",
       "3  07h22m14.39s  110.559945  -25d12m25.26s  ...        NaN   1177.36000   \n",
       "4  08h08m42.77s  122.178195  -48d48m10.12s  ...  11.311300  54679.30000   \n",
       "\n",
       "        pl_eqt   st_tmag   st_dist  st_teff  st_logg   st_rad  \\\n",
       "0  3127.204052  9.604000   485.735  10249.0     4.19  2.16986   \n",
       "1  4045.000000  9.423440   295.862   7070.0     4.03  2.01000   \n",
       "2  2037.000000  9.299501   943.109   8924.0      NaN  5.73000   \n",
       "3  1631.000000  9.300300  7728.170   5388.5     4.15      NaN   \n",
       "4  4260.000000  9.135500   356.437   9219.0     4.14  2.15000   \n",
       "\n",
       "        toi_created         rowupdate  \n",
       "0  24/07/2019 15:58  09/09/2024 10:08  \n",
       "1  24/07/2019 15:58  03/04/2023 14:31  \n",
       "2  24/07/2019 15:58  11/07/2022 16:02  \n",
       "3  24/07/2019 15:58  23/02/2022 10:10  \n",
       "4  24/07/2019 15:58  09/09/2024 10:08  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fixed get_dataset_path function - corrected \"Note books\" to \"Notebooks\"\n",
    "def get_dataset_path_fixed(dataset_name='TESS'):\n",
    "    mapping = {\n",
    "        'TESS': r'Data Sources\\TESS.csv',\n",
    "        # add others if needed\n",
    "    }\n",
    "    return mapping.get(dataset_name)\n",
    "\n",
    "# Use the fixed function\n",
    "DATA_PATH = get_dataset_path_fixed('TESS')\n",
    "if DATA_PATH is None or not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"TESS.csv not found at {DATA_PATH}. Please check the file path in the get_dataset_path function.\")\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print('Loaded TESS dataset, shape:', df_raw.shape)\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd43754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories prepared: C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\models C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\plots C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\results\n"
     ]
    }
   ],
   "source": [
    "# Fixed directory paths - corrected \"Note books\" to \"Notebooks\"\n",
    "BASE_MODEL_DIR = r'static\\models'\n",
    "PLOTS_DIR = r'static\\plots'\n",
    "RESULTS_DIR = r'static\\results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(BASE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Directories prepared: {BASE_MODEL_DIR} {PLOTS_DIR} {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca8ed1",
   "metadata": {},
   "source": [
    "## Step 2 — Dataset Path Helper\n",
    "Map dataset name to path (TESS is default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_path(dataset_name='TESS'):\n",
    "    mapping = {\n",
    "        'TESS': r\"Data Sources\\TESS.csv\",\n",
    "        # add others if needed\n",
    "    }\n",
    "    return mapping.get(dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf872e4",
   "metadata": {},
   "source": [
    "## Step 3 — Load & Inspect Data\n",
    "Load TESS.csv and show basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "110d9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_dataset_path(dataset_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the full path to a dataset CSV file.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset (e.g., 'Kepler', 'TESS').\n",
    "    \n",
    "    Returns:\n",
    "        str: Full path to the dataset file.\n",
    "    \"\"\"\n",
    "    # Base data folder (adjust if needed)\n",
    "    base_dir = r\"C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\Data Sources\"\n",
    "    \n",
    "    # Map dataset names to filenames\n",
    "    dataset_files = {\n",
    "        \"Kepler\": \"Kepler.csv\",\n",
    "        \"TESS\": \"TESS.csv\"\n",
    "    }\n",
    "    \n",
    "    # Return path if exists\n",
    "    if dataset_name in dataset_files:\n",
    "        return os.path.join(base_dir, dataset_files[dataset_name])\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6208b",
   "metadata": {},
   "source": [
    "## Step 4 — Feature Engineering & Renaming\n",
    "Select relevant columns and rename to readable names. Update mapping if your CSV uses different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a6d1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution: {1: 6408, 0: 1295}\n",
      "After selection & renaming, shape: (7703, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_radius_earth</th>\n",
       "      <th>transit_depth_ppm</th>\n",
       "      <th>orbital_period_days</th>\n",
       "      <th>transit_duration_hrs</th>\n",
       "      <th>insolation_flux_earth</th>\n",
       "      <th>equilibrium_temp_k</th>\n",
       "      <th>stellar_temp_k</th>\n",
       "      <th>stellar_logg</th>\n",
       "      <th>stellar_radius_solar</th>\n",
       "      <th>stellar_magnitude</th>\n",
       "      <th>stellar_distance_pc</th>\n",
       "      <th>RA_deg</th>\n",
       "      <th>Dec_deg</th>\n",
       "      <th>disposition</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.818163</td>\n",
       "      <td>656.886099</td>\n",
       "      <td>2.171348</td>\n",
       "      <td>2.01722</td>\n",
       "      <td>22601.94858</td>\n",
       "      <td>3127.204052</td>\n",
       "      <td>10249.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>2.16986</td>\n",
       "      <td>9.604000</td>\n",
       "      <td>485.735</td>\n",
       "      <td>112.357708</td>\n",
       "      <td>-12.695960</td>\n",
       "      <td>FP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.215400</td>\n",
       "      <td>1286.000000</td>\n",
       "      <td>1.931646</td>\n",
       "      <td>3.16600</td>\n",
       "      <td>44464.50000</td>\n",
       "      <td>4045.000000</td>\n",
       "      <td>7070.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.01000</td>\n",
       "      <td>9.423440</td>\n",
       "      <td>295.862</td>\n",
       "      <td>122.580465</td>\n",
       "      <td>-5.513852</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.752900</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1.867557</td>\n",
       "      <td>1.40800</td>\n",
       "      <td>2860.61000</td>\n",
       "      <td>2037.000000</td>\n",
       "      <td>8924.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.73000</td>\n",
       "      <td>9.299501</td>\n",
       "      <td>943.109</td>\n",
       "      <td>104.726966</td>\n",
       "      <td>-10.580455</td>\n",
       "      <td>FP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>383.410000</td>\n",
       "      <td>2.743230</td>\n",
       "      <td>3.16700</td>\n",
       "      <td>1177.36000</td>\n",
       "      <td>1631.000000</td>\n",
       "      <td>5388.5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.300300</td>\n",
       "      <td>7728.170</td>\n",
       "      <td>110.559945</td>\n",
       "      <td>-25.207017</td>\n",
       "      <td>FP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.311300</td>\n",
       "      <td>755.000000</td>\n",
       "      <td>3.573014</td>\n",
       "      <td>3.37000</td>\n",
       "      <td>54679.30000</td>\n",
       "      <td>4260.000000</td>\n",
       "      <td>9219.0</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.15000</td>\n",
       "      <td>9.135500</td>\n",
       "      <td>356.437</td>\n",
       "      <td>122.178195</td>\n",
       "      <td>-48.802811</td>\n",
       "      <td>FP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   planet_radius_earth  transit_depth_ppm  orbital_period_days  \\\n",
       "0             5.818163         656.886099             2.171348   \n",
       "1            11.215400        1286.000000             1.931646   \n",
       "2            23.752900        1500.000000             1.867557   \n",
       "3                  NaN         383.410000             2.743230   \n",
       "4            11.311300         755.000000             3.573014   \n",
       "\n",
       "   transit_duration_hrs  insolation_flux_earth  equilibrium_temp_k  \\\n",
       "0               2.01722            22601.94858         3127.204052   \n",
       "1               3.16600            44464.50000         4045.000000   \n",
       "2               1.40800             2860.61000         2037.000000   \n",
       "3               3.16700             1177.36000         1631.000000   \n",
       "4               3.37000            54679.30000         4260.000000   \n",
       "\n",
       "   stellar_temp_k  stellar_logg  stellar_radius_solar  stellar_magnitude  \\\n",
       "0         10249.0          4.19               2.16986           9.604000   \n",
       "1          7070.0          4.03               2.01000           9.423440   \n",
       "2          8924.0           NaN               5.73000           9.299501   \n",
       "3          5388.5          4.15                   NaN           9.300300   \n",
       "4          9219.0          4.14               2.15000           9.135500   \n",
       "\n",
       "   stellar_distance_pc      RA_deg    Dec_deg disposition  Target  \n",
       "0              485.735  112.357708 -12.695960          FP       0  \n",
       "1              295.862  122.580465  -5.513852          PC       1  \n",
       "2              943.109  104.726966 -10.580455          FP       0  \n",
       "3             7728.170  110.559945 -25.207017          FP       0  \n",
       "4              356.437  122.178195 -48.802811          FP       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a copy and normalize column names\n",
    "df = df_raw.copy()\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# TESS dataset column selection & rename dictionary\n",
    "selected = [\n",
    "    # physical / transit properties\n",
    "    \"pl_rade\", \"pl_trandep\", \"pl_orbper\", \"pl_trandurh\", \n",
    "    \"pl_insol\", \"pl_eqt\",\n",
    "    # stellar properties\n",
    "    \"st_teff\", \"st_logg\", \"st_rad\", \"st_tmag\", \"st_dist\",\n",
    "    # coordinates\n",
    "    \"ra\", \"dec\",\n",
    "    # disposition\n",
    "    \"tfopwg_disp\"\n",
    "]\n",
    "\n",
    "rename_map = {\n",
    "    \"pl_rade\": \"planet_radius_earth\",\n",
    "    \"pl_trandep\": \"transit_depth_ppm\", \n",
    "    \"pl_orbper\": \"orbital_period_days\",\n",
    "    \"pl_trandurh\": \"transit_duration_hrs\",\n",
    "    \"pl_insol\": \"insolation_flux_earth\",\n",
    "    \"pl_eqt\": \"equilibrium_temp_k\",\n",
    "    \"st_teff\": \"stellar_temp_k\",\n",
    "    \"st_logg\": \"stellar_logg\",\n",
    "    \"st_rad\": \"stellar_radius_solar\",\n",
    "    \"st_tmag\": \"stellar_magnitude\",\n",
    "    \"st_dist\": \"stellar_distance_pc\",\n",
    "    \"ra\": \"RA_deg\",\n",
    "    \"dec\": \"Dec_deg\",\n",
    "    \"tfopwg_disp\": \"disposition\"\n",
    "}\n",
    "\n",
    "# Keep only those columns that exist in the dataframe\n",
    "cols = [c for c in selected if c in df.columns]\n",
    "df = df[cols].rename(columns=rename_map)\n",
    "\n",
    "# Create Target column based on TESS disposition values\n",
    "# PC (Planet Candidate), CP (Confirmed Planet), KP (Kepler Planet), APC (Ambiguous Planet Candidate) = 1 (positive)\n",
    "# FP (False Positive), FA (False Alarm) = 0 (negative)\n",
    "if 'disposition' in df.columns:\n",
    "    # Filter to keep only valid disposition values\n",
    "    valid_dispositions = ['PC', 'FP', 'CP', 'KP', 'APC', 'FA']\n",
    "    df = df[df['disposition'].isin(valid_dispositions)].copy()\n",
    "    \n",
    "    # Create Target: 1 for planet candidates/confirmed, 0 for false positives\n",
    "    df['Target'] = df['disposition'].isin(['PC', 'CP', 'KP', 'APC']).astype(int)\n",
    "    print(f\"Target distribution: {df['Target'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print('Warning: disposition column not found — please verify column name and edit the notebook accordingly.')\n",
    "\n",
    "print('After selection & renaming, shape:', df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228609ab",
   "metadata": {},
   "source": [
    "## Step 5 — Missing Value Handling & Conservative Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4e0f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls after imputation: 0\n",
      "Shape before outlier removal: (7703, 15) after: (7666, 15)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing numeric with median, categorical with mode\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for c in numeric_cols:\n",
    "    if df[c].isnull().any():\n",
    "        df[c].fillna(df[c].median(), inplace=True)\n",
    "for c in categorical_cols:\n",
    "    if df[c].isnull().any():\n",
    "        df[c].fillna(df[c].mode().iloc[0], inplace=True)\n",
    "\n",
    "print('Nulls after imputation:', df.isnull().sum().sum())\n",
    "\n",
    "# Conservative outlier removal using 1st/99th percentiles and 3*IQR\n",
    "def remove_extreme_outliers(df, numeric_cols, lower_q=0.01, upper_q=0.99, multiplier=3.0):\n",
    "    q_low = df[numeric_cols].quantile(lower_q)\n",
    "    q_high = df[numeric_cols].quantile(upper_q)\n",
    "    iqr = q_high - q_low\n",
    "    lower = q_low - multiplier * iqr\n",
    "    upper = q_high + multiplier * iqr\n",
    "    mask = ~((df[numeric_cols] < lower) | (df[numeric_cols] > upper)).any(axis=1)\n",
    "    return df[mask]\n",
    "\n",
    "num_cols = [c for c in numeric_cols if c != 'Target']\n",
    "if len(num_cols) > 0:\n",
    "    df_clean = remove_extreme_outliers(df, num_cols, lower_q=0.01, upper_q=0.99, multiplier=3.0)\n",
    "    print('Shape before outlier removal:', df.shape, 'after:', df_clean.shape)\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print('No numeric columns to apply outlier removal.')\n",
    "\n",
    "df = df_clean.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98dcb7",
   "metadata": {},
   "source": [
    "## Step 6 — Train/Val/Test Split & Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a38950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> train: (4599, 14) val: (1533, 14) test: (1534, 14)\n",
      "Preprocessor built. Numeric features: 13 Categorical: 1\n"
     ]
    }
   ],
   "source": [
    "# Ensure Target exists\n",
    "if 'Target' not in df.columns:\n",
    "    raise ValueError(\"Target column not found — set Target creation logic in the Feature Engineering step.\")\n",
    "\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target'].astype(int)\n",
    "\n",
    "# train/val/test split (60/20/20)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=RANDOM_STATE, stratify=y_train_val)\n",
    "\n",
    "print('Shapes -> train:', X_train.shape, 'val:', X_val.shape, 'test:', X_test.shape)\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def build_preprocessor(numeric_features, categorical_features):\n",
    "    num_pipe = Pipeline([('scaler', StandardScaler())])\n",
    "    cat_pipe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) if len(categorical_features)>0 else None\n",
    "    transformers = []\n",
    "    if len(numeric_features)>0:\n",
    "        transformers.append(('num', num_pipe, numeric_features))\n",
    "    if cat_pipe is not None:\n",
    "        transformers.append(('cat', cat_pipe, categorical_features))\n",
    "    preprocessor = ColumnTransformer(transformers, remainder='drop')\n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = build_preprocessor(numeric_features, categorical_features)\n",
    "print('Preprocessor built. Numeric features:', len(numeric_features), 'Categorical:', len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3c811",
   "metadata": {},
   "source": [
    "## Step 7 — Train Models (RandomForest, XGBoost if available, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2381626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RandomForest -> C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\models\\RandomForest_pipeline.pkl\n",
      "Training LogisticRegression\n",
      "Saved LogisticRegression -> C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\models\\LogisticRegression_pipeline.pkl\n",
      "Training XGBoost\n",
      "Saved XGBoost -> C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\models\\XGBoost_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "if XGB_AVAILABLE:\n",
    "    models['XGBoost'] = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=RANDOM_STATE)\n",
    "\n",
    "trained_models = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('clf', clf)])\n",
    "    print('Training', name)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    path = os.path.join(BASE_MODEL_DIR, f'{name}_pipeline.pkl')\n",
    "    joblib.dump(pipe, path)\n",
    "    trained_models[name] = pipe\n",
    "    print('Saved', name, '->', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10994b09",
   "metadata": {},
   "source": [
    "## Step 8 — Evaluate Models & Save Metrics/Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8738303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\results\\TESS_metrics.json\n",
      "Saved comparison plot to C:\\Users\\Abdelrahman Bakr\\Desktop\\me\\project\\Nasa\\Exoplanets-Detection-Using-Machine-Learning\\Backend\\Notebooks\\static\\plots\\TESS_model_comparison.png\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models_dict, X_eval, y_eval, dataset_name='TESS'):\n",
    "    results = {}\n",
    "    for name, model in models_dict.items():\n",
    "        y_pred = model.predict(X_eval)\n",
    "        y_proba = model.predict_proba(X_eval)[:,1] if hasattr(model, 'predict_proba') else None\n",
    "        metrics = {\n",
    "            'accuracy': float(accuracy_score(y_eval, y_pred)),\n",
    "            'precision': float(precision_score(y_eval, y_pred, zero_division=0)),\n",
    "            'recall': float(recall_score(y_eval, y_pred, zero_division=0)),\n",
    "            'f1': float(f1_score(y_eval, y_pred, zero_division=0)),\n",
    "            'auc': float(roc_auc_score(y_eval, y_proba)) if y_proba is not None else None\n",
    "        }\n",
    "        results[name] = metrics\n",
    "\n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(y_eval, y_pred)\n",
    "        plt.figure(figsize=(4,3))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
    "        plt.title(f'{dataset_name} - {name} Confusion')\n",
    "        cm_path = os.path.join(PLOTS_DIR, f'{dataset_name}_{name}_confusion.png')\n",
    "        plt.savefig(cm_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # ROC\n",
    "        if y_proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_eval, y_proba)\n",
    "            plt.figure(figsize=(5,4))\n",
    "            plt.plot(fpr, tpr, label=f'AUC={metrics[\"auc\"]:.3f}')\n",
    "            plt.plot([0,1],[0,1],'--')\n",
    "            plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'{dataset_name} - {name} ROC')\n",
    "            roc_path = os.path.join(PLOTS_DIR, f'{dataset_name}_{name}_roc.png')\n",
    "            plt.savefig(roc_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    out_path = os.path.join(RESULTS_DIR, f'{dataset_name}_metrics.json')\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print('Saved metrics to', out_path)\n",
    "    return results\n",
    "\n",
    "metrics = evaluate_models(trained_models, X_val, y_val, dataset_name='TESS')\n",
    "\n",
    "# Comparison barplot\n",
    "res_df = pd.DataFrame(metrics).T.reset_index().rename(columns={'index':'model'})\n",
    "melted = res_df.melt(id_vars='model', value_vars=[c for c in ['accuracy','precision','recall','f1','auc'] if c in res_df.columns],\n",
    "                     var_name='metric', value_name='value')\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=melted, x='model', y='value', hue='metric')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "comp_path = os.path.join(PLOTS_DIR, 'TESS_model_comparison.png')\n",
    "plt.savefig(comp_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved comparison plot to', comp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967d2d6",
   "metadata": {},
   "source": [
    "## Step 9 — Extract Top-5 Features per Model & Save Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a39d144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: importance length 19 != feature len 14 for RandomForest\n",
      "Saved top features for LogisticRegression\n",
      "Warning: importance length 19 != feature len 14 for XGBoost\n",
      "Top features saved: {'LogisticRegression': ['disposition', 'equilibrium_temp_k', 'stellar_radius_solar', 'planet_radius_earth', 'stellar_magnitude']}\n"
     ]
    }
   ],
   "source": [
    "def extract_top_features_and_save(models_dict, X_train_df, dataset_name='TESS', top_k=5):\n",
    "    feature_names = X_train_df.columns.tolist()\n",
    "    medians = X_train_df.median(numeric_only=True).to_dict()\n",
    "    with open(os.path.join(RESULTS_DIR, f'{dataset_name}_feature_medians.json'), 'w') as f:\n",
    "        json.dump(medians, f, indent=2)\n",
    "    # save training columns order\n",
    "    with open(os.path.join(RESULTS_DIR, f'{dataset_name}_training_columns.json'), 'w') as f:\n",
    "        json.dump(feature_names, f, indent=2)\n",
    "\n",
    "    all_top = {}\n",
    "    for name, model in models_dict.items():\n",
    "        clf = model.named_steps['clf']\n",
    "        imp = None\n",
    "        if hasattr(clf, 'feature_importances_'):\n",
    "            imp = np.array(clf.feature_importances_)\n",
    "        elif hasattr(clf, 'coef_'):\n",
    "            arr = np.array(clf.coef_)\n",
    "            imp = np.abs(arr.ravel())[:len(feature_names)]\n",
    "        else:\n",
    "            print('No importances for', name)\n",
    "            continue\n",
    "\n",
    "        if len(imp) != len(feature_names):\n",
    "            print(f'Warning: importance length {len(imp)} != feature len {len(feature_names)} for {name}')\n",
    "            # try to skip or map where possible\n",
    "            continue\n",
    "\n",
    "        s = pd.Series(imp, index=feature_names).sort_values(ascending=False)\n",
    "        top_feats = s.head(top_k).index.tolist()\n",
    "        all_top[name] = top_feats\n",
    "        with open(os.path.join(RESULTS_DIR, f'{dataset_name}_{name}_top_features.json'), 'w') as f:\n",
    "            json.dump(top_feats, f, indent=2)\n",
    "\n",
    "        # plot top features\n",
    "        plt.figure(figsize=(6, max(2, len(top_feats)*0.5)))\n",
    "        sns.barplot(x=s.head(top_k).values, y=s.head(top_k).index)\n",
    "        plt.title(f'{name} Top {top_k} features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f'{dataset_name}_{name}_topk.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print('Saved top features for', name)\n",
    "    return all_top\n",
    "\n",
    "top_features = extract_top_features_and_save(trained_models, X_train, dataset_name='TESS', top_k=5)\n",
    "print('Top features saved:', top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc29a44",
   "metadata": {},
   "source": [
    "## Step 10 — Prediction Helpers for Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83b2ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_full_vector(model_name, input_vector):\n",
    "    model_path = os.path.join(BASE_MODEL_DIR, f'{model_name}_pipeline.pkl')\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f'Model not found: {model_path}')\n",
    "    model = joblib.load(model_path)\n",
    "    X = np.array(input_vector).reshape(1, -1)\n",
    "    pred = int(model.predict(X)[0])\n",
    "    proba = float(model.predict_proba(X)[0][1]) if hasattr(model, 'predict_proba') else None\n",
    "    return {'prediction': pred, 'probability': proba}\n",
    "\n",
    "def predict_from_top5(model_name, dataset_name, top5_mapping):\n",
    "    # top5_mapping: dict {feature_name: value} for the model's top5 order\n",
    "    med_path = os.path.join(RESULTS_DIR, f'{dataset_name}_feature_medians.json')\n",
    "    cols_path = os.path.join(RESULTS_DIR, f'{dataset_name}_training_columns.json')\n",
    "    top_path = os.path.join(RESULTS_DIR, f'{dataset_name}_{model_name}_top_features.json')\n",
    "    if not (os.path.exists(med_path) and os.path.exists(cols_path) and os.path.exists(top_path)):\n",
    "        raise FileNotFoundError('Required artifacts (medians/cols/top_features) missing. Run training workflow first.')\n",
    "    with open(med_path,'r') as f:\n",
    "        medians = json.load(f)\n",
    "    with open(cols_path,'r') as f:\n",
    "        cols = json.load(f)\n",
    "    with open(top_path,'r') as f:\n",
    "        top_feats = json.load(f)\n",
    "    # start with medians\n",
    "    row = {c: medians.get(c, 0.0) for c in cols}\n",
    "    # overwrite with provided top5\n",
    "    for k,v in top5_mapping.items():\n",
    "        if k not in row:\n",
    "            raise ValueError(f'Feature {k} not in training columns')\n",
    "        row[k] = v\n",
    "    df_row = pd.DataFrame([row], columns=cols)\n",
    "    model = joblib.load(os.path.join(BASE_MODEL_DIR, f'{model_name}_pipeline.pkl'))\n",
    "    pred = int(model.predict(df_row)[0])\n",
    "    proba = float(model.predict_proba(df_row)[0][1]) if hasattr(model, 'predict_proba') else None\n",
    "    return {'prediction': pred, 'probability': proba}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866f372",
   "metadata": {},
   "source": [
    "## Step 11 — Run Full Workflow (example)\n",
    "Run `run_full_workflow()` to execute the end-to-end pipeline for TESS. It will save artifacts for Flask consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa34c0",
   "metadata": {},
   "source": [
    "## Step 13 — Model Results Analysis Function\n",
    "\n",
    "Comprehensive function to analyze and display modeling results with visualizations and detailed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98d62005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_workflow(dataset_name='TESS'):\n",
    "    path = get_dataset_path(dataset_name)\n",
    "    if path is None or not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'Dataset not found at {path}')\n",
    "    df0 = pd.read_csv(path)\n",
    "    print('Initial shape:', df0.shape)\n",
    "    # reuse logic above: normalize column names and select/rename\n",
    "    df0.columns = [c.strip() for c in df0.columns]\n",
    "    # apply same selection & rename as earlier in notebook\n",
    "    # to keep notebook compact, call the steps we defined earlier in cells: we will rebuild minimal selection here\n",
    "    df1 = df0.copy()\n",
    "    # attempt to recreate same processing: only keep columns present in df\n",
    "    cols_present = [c for c in ['pl_rade','pl_trandep','pl_orbper','pl_trandurh','pl_insol','pl_eqt','st_teff','st_logg','st_rad','st_tmag','st_dist','ra','dec','tfopwg_disp'] if c in df1.columns]\n",
    "    df_sel = df1[cols_present].copy()\n",
    "    # rename where possible\n",
    "    df_sel = df_sel.rename(columns={k: v for k,v in {\n",
    "        'pl_rade': 'planet_radius_earth','pl_trandep':'transit_depth_ppm','pl_orbper':'orbital_period_days','pl_trandurh':'transit_duration_hrs',\n",
    "        'pl_insol':'insolation_flux_earth','pl_eqt':'equilibrium_temp_k','st_teff':'stellar_temp_k','st_logg':'stellar_logg',\n",
    "        'st_rad':'stellar_radius_solar','st_tmag':'stellar_magnitude','st_dist':'stellar_distance_pc','ra':'RA_deg','dec':'Dec_deg','tfopwg_disp':'disposition'\n",
    "    }.items() if k in df1.columns})\n",
    "    # create Target if possible\n",
    "    if 'disposition' in df_sel.columns:\n",
    "        valid_dispositions = ['PC', 'FP', 'CP', 'KP', 'APC', 'FA']\n",
    "        df_sel = df_sel[df_sel['disposition'].isin(valid_dispositions)].copy()\n",
    "        df_sel['Target'] = df_sel['disposition'].isin(['PC', 'CP', 'KP', 'APC']).astype(int)\n",
    "    else:\n",
    "        raise ValueError('disposition not found in dataset; cannot create Target automatically.')\n",
    "    print('Selected & filtered shape:', df_sel.shape)\n",
    "    # proceed with imputation and outlier removal (reuse above)\n",
    "    numeric_cols = df_sel.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for c in numeric_cols:\n",
    "        df_sel[c].fillna(df_sel[c].median(), inplace=True)\n",
    "    categorical_cols = df_sel.select_dtypes(include=['object']).columns.tolist()\n",
    "    for c in categorical_cols:\n",
    "        df_sel[c].fillna(df_sel[c].mode().iloc[0], inplace=True)\n",
    "    df_clean = remove_extreme_outliers(df_sel, [c for c in numeric_cols if c!='Target'], lower_q=0.01, upper_q=0.99, multiplier=3.0)\n",
    "    df_clean = df_clean.dropna(subset=['Target'])\n",
    "    print('After cleaning shape:', df_clean.shape)\n",
    "    # save training columns and medians\n",
    "    training_cols = [c for c in df_clean.columns if c!='Target']\n",
    "    with open(os.path.join(RESULTS_DIR, 'TESS_training_columns.json'), 'w') as f:\n",
    "        json.dump(training_cols, f, indent=2)\n",
    "    medians = df_clean[training_cols].median(numeric_only=True).to_dict()\n",
    "    with open(os.path.join(RESULTS_DIR, 'TESS_feature_medians.json'), 'w') as f:\n",
    "        json.dump(medians, f, indent=2)\n",
    "    # split & train\n",
    "    X = df_clean[training_cols]\n",
    "    y = df_clean['Target'].astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    preprocessor = build_preprocessor(X_train.select_dtypes(include=[np.number]).columns.tolist(), X_train.select_dtypes(include=['object']).columns.tolist())\n",
    "    # train models\n",
    "    trained = {}\n",
    "    model_defs = {'RandomForest': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE), 'LogisticRegression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)}\n",
    "    if XGB_AVAILABLE:\n",
    "        model_defs['XGBoost'] = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=RANDOM_STATE)\n",
    "    for name, clf in model_defs.items():\n",
    "        pipe = Pipeline([('preprocessor', preprocessor), ('clf', clf)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        joblib.dump(pipe, os.path.join(BASE_MODEL_DIR, f'{name}_pipeline.pkl'))\n",
    "        trained[name] = pipe\n",
    "    # evaluate\n",
    "    metrics = evaluate_models(trained, X_test, y_test, dataset_name='TESS')\n",
    "    # extract top features\n",
    "    extract_top_features_and_save(trained, X_train, dataset_name='TESS', top_k=5)\n",
    "    print('Workflow finished for TESS. Artifacts in /mnt/data/static/')\n",
    "    return metrics\n",
    "\n",
    "# Note: Running run_full_workflow() will train models and save artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbad04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_results(trained_models, X_test, y_test, X_train, y_train, dataset_name='TESS'):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of model results with detailed metrics, visualizations, and comparisons.\n",
    "    \n",
    "    Parameters:\n",
    "    - trained_models: dict of trained model pipelines\n",
    "    - X_test, y_test: test set for evaluation\n",
    "    - X_train, y_train: training set for analysis\n",
    "    - dataset_name: name for saving files\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                                roc_curve, precision_recall_curve, \n",
    "                                average_precision_score)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE MODEL ANALYSIS FOR {dataset_name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    probabilities = {}\n",
    "    \n",
    "    # 1. INDIVIDUAL MODEL ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, model in trained_models.items():\n",
    "        print(f\"\\n--- {model_name.upper()} ---\")\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        predictions[model_name] = y_pred\n",
    "        probabilities[model_name] = y_proba\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "        }\n",
    "        \n",
    "        results[model_name] = metrics\n",
    "        \n",
    "        # Print detailed metrics\n",
    "        print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
    "        if metrics['auc']:\n",
    "            print(f\"AUC-ROC:   {metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['False Positive', 'Exoplanet']))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(f\"True Negatives:  {cm[0,0]:4d} | False Positives: {cm[0,1]:4d}\")\n",
    "        print(f\"False Negatives: {cm[1,0]:4d} | True Positives:  {cm[1,1]:4d}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model.named_steps['clf'], 'feature_importances_'):\n",
    "            feature_names = X_train.columns.tolist()\n",
    "            importances = model.named_steps['clf'].feature_importances_\n",
    "            feature_importance = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "            print(f\"\\nTop 5 Most Important Features:\")\n",
    "            for i, (feature, importance) in enumerate(feature_importance.head().items()):\n",
    "                print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # 2. MODEL COMPARISON\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(results).T\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Find best model for each metric\n",
    "    print(f\"\\nBest Models:\")\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "        if metric in comparison_df.columns and not comparison_df[metric].isna().all():\n",
    "            best_model = comparison_df[metric].idxmax()\n",
    "            best_score = comparison_df.loc[best_model, metric]\n",
    "            print(f\"Best {metric.upper()}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # 3. VISUALIZATIONS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # 3.1 Model Comparison Bar Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'{dataset_name} - Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    comparison_df['accuracy'].plot(kind='bar', ax=axes[0,0], title='Accuracy Comparison')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    comparison_df['f1'].plot(kind='bar', ax=axes[0,1], title='F1-Score Comparison')\n",
    "    axes[0,1].set_ylabel('F1-Score')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Precision comparison\n",
    "    comparison_df['precision'].plot(kind='bar', ax=axes[1,0], title='Precision Comparison')\n",
    "    axes[1,0].set_ylabel('Precision')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall comparison\n",
    "    comparison_df['recall'].plot(kind='bar', ax=axes[1,1], title='Recall Comparison')\n",
    "    axes[1,1].set_ylabel('Recall')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    comparison_plot_path = os.path.join(PLOTS_DIR, f'{dataset_name}_model_comparison_detailed.png')\n",
    "    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved detailed comparison plot: {comparison_plot_path}\")\n",
    "    \n",
    "    # 3.2 ROC Curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, y_proba in probabilities.items():\n",
    "        if y_proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            auc_score = roc_auc_score(y_test, y_proba)\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{dataset_name} - ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    roc_plot_path = os.path.join(PLOTS_DIR, f'{dataset_name}_roc_curves.png')\n",
    "    plt.savefig(roc_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved ROC curves plot: {roc_plot_path}\")\n",
    "    \n",
    "    # 3.3 Confusion Matrices\n",
    "    n_models = len(trained_models)\n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "        row, col = i // cols, i % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=['False Positive', 'Exoplanet'],\n",
    "                   yticklabels=['False Positive', 'Exoplanet'])\n",
    "        ax.set_title(f'{model_name} Confusion Matrix', fontweight='bold')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_models, rows * cols):\n",
    "        row, col = i // cols, i % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_plot_path = os.path.join(PLOTS_DIR, f'{dataset_name}_confusion_matrices.png')\n",
    "    plt.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved confusion matrices plot: {cm_plot_path}\")\n",
    "    \n",
    "    # 4. DATASET ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Total samples: {len(X_train) + len(X_test)}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "    test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(f\"Training set - False Positive: {train_dist[0]}, Exoplanet: {train_dist[1]}\")\n",
    "    print(f\"Test set - False Positive: {test_dist[0]}, Exoplanet: {test_dist[1]}\")\n",
    "    \n",
    "    # Feature analysis\n",
    "    print(f\"\\nFeature Analysis:\")\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "    print(f\"Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    if len(numeric_features) > 0:\n",
    "        print(f\"\\nNumeric features: {list(numeric_features)}\")\n",
    "    if len(categorical_features) > 0:\n",
    "        print(f\"Categorical features: {list(categorical_features)}\")\n",
    "    \n",
    "    # 5. SAVE RESULTS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Save detailed results\n",
    "    detailed_results = {\n",
    "        'dataset_info': {\n",
    "            'name': dataset_name,\n",
    "            'total_samples': len(X_train) + len(X_test),\n",
    "            'train_samples': len(X_train),\n",
    "            'test_samples': len(X_test),\n",
    "            'features': X_train.shape[1],\n",
    "            'numeric_features': len(numeric_features),\n",
    "            'categorical_features': len(categorical_features)\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'train': train_dist.to_dict(),\n",
    "            'test': test_dist.to_dict()\n",
    "        },\n",
    "        'model_performance': results,\n",
    "        'best_models': {\n",
    "            metric: comparison_df[metric].idxmax() \n",
    "            for metric in comparison_df.columns \n",
    "            if not comparison_df[metric].isna().all()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(RESULTS_DIR, f'{dataset_name}_detailed_analysis.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Saved detailed analysis: {results_path}\")\n",
    "    print(f\"Saved plots in: {PLOTS_DIR}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return detailed_results, comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f692f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis function ready! Uncomment the lines above to run comprehensive model analysis.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the analysis function\n",
    "# Run this after training your models to get comprehensive results\n",
    "\n",
    "# detailed_results, comparison_df = analyze_model_results(\n",
    "#     trained_models=trained_models,\n",
    "#     X_test=X_test, \n",
    "#     y_test=y_test,\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     dataset_name='TESS'\n",
    "# )\n",
    "\n",
    "print(\"Analysis function ready! Uncomment the lines above to run comprehensive model analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73086580",
   "metadata": {},
   "source": [
    "## Step 12 — Final Summary\n",
    "After running, models will be saved to `/mnt/data/static/models`, plots to `/mnt/data/static/plots`, and metrics/top-features to `/mnt/data/static/results`. Use these artifacts in your Flask backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
