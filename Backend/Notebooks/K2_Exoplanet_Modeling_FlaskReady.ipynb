{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94ff55c",
   "metadata": {
    "id": "a94ff55c"
   },
   "source": [
    "# K2 Exoplanet Modeling (Flask-ready)\n",
    "\n",
    "**Purpose:** Modular notebook for loading K2 dataset, training models, saving artifacts for a Flask app, and providing prediction utilities that accept top-5 feature values from the web UI.\n",
    "\n",
    "This notebook follows a structured workflow with clear steps and saves models/plots/results to `../static/` so Flask can serve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc422d",
   "metadata": {
    "id": "17cc422d"
   },
   "source": [
    "## Step 1 — Imports & Global Configuration\n",
    "Import libraries and define global directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfc71da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:51:55.206660Z",
     "iopub.status.busy": "2025-10-05T21:51:55.205683Z",
     "iopub.status.idle": "2025-10-05T21:52:52.118091Z",
     "shell.execute_reply": "2025-10-05T21:52:52.114818Z"
    },
    "id": "8dfc71da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, roc_curve)\n",
    "import joblib\n",
    "\n",
    "# Directories for Flask to read\n",
    "BASE_MODEL_DIR = '../static/models'\n",
    "PLOTS_DIR = '../static/plots'\n",
    "RESULTS_DIR = '../static/results'\n",
    "DATA_DIR = '../../Data Sources'  # expected location for K2.csv\n",
    "\n",
    "os.makedirs(BASE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7d20a",
   "metadata": {
    "id": "5ae7d20a"
   },
   "source": [
    "## Step 2 — Dataset Path Helper\n",
    "Map dataset names to file paths. Ensure `K2.csv` is placed under `../../Data Sources/` or you may point to `/mnt/data/k2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f47ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.130164Z",
     "iopub.status.busy": "2025-10-05T21:52:52.128809Z",
     "iopub.status.idle": "2025-10-05T21:52:52.142478Z",
     "shell.execute_reply": "2025-10-05T21:52:52.139885Z"
    },
    "id": "e5f47ff3"
   },
   "outputs": [],
   "source": [
    "def get_dataset_path(dataset_name):\n",
    "    dataset_map = {\n",
    "        'K2': os.path.join('..','..','Data Sources','K2.csv'),\n",
    "        'local_k2': r\"Data Sources\\k2.csv\" # Added mapping for local k2.csv\n",
    "    }\n",
    "    return dataset_map.get(dataset_name, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba125f8",
   "metadata": {
    "id": "8ba125f8"
   },
   "source": [
    "## Step 3 — Load, Select Columns & Rename\n",
    "Load the K2 dataset, select relevant columns and create the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bbc439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.153232Z",
     "iopub.status.busy": "2025-10-05T21:52:52.152147Z",
     "iopub.status.idle": "2025-10-05T21:52:52.173688Z",
     "shell.execute_reply": "2025-10-05T21:52:52.170764Z"
    },
    "id": "27bbc439"
   },
   "outputs": [],
   "source": [
    "def load_raw_dataset(csv_path):\n",
    "    if csv_path is None:\n",
    "        raise FileNotFoundError('Dataset path is None')\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f'File not found: {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f'Loaded {csv_path} -> shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "def preprocess_initial(df):\n",
    "    # Basic cleaning: strip column names\n",
    "    df = df.rename(columns=lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "def select_and_prepare_k2(df):\n",
    "    # Keep a conservative set of useful columns (based on your earlier notebook)\n",
    "    keep = [\n",
    "        'pl_orbper','pl_rade','pl_radj','st_rad','st_mass','sy_dist',\n",
    "        'st_teff','st_logg','disposition','discoverymethod','disc_facility','soltype','pl_name'\n",
    "    ]\n",
    "    existing = [c for c in keep if c in df.columns]\n",
    "    df_sel = df[existing].copy()\n",
    "    # Drop rows with no disposition info\n",
    "    if 'disposition' in df_sel.columns:\n",
    "        df_sel = df_sel[df_sel['disposition'].notna()]\n",
    "        # map dispositions\n",
    "        df_sel = df_sel[df_sel['disposition'] != 'REFUTED']\n",
    "        df_sel['Target'] = df_sel['disposition'].map({'CONFIRMED':2,'CANDIDATE':1,'FALSE POSITIVE':0})\n",
    "    else:\n",
    "        raise KeyError('disposition column not found in K2 dataset')\n",
    "    # Fill simple missing numeric values with median\n",
    "    num_cols = df_sel.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "    for c in num_cols:\n",
    "        df_sel[c] = df_sel[c].fillna(df_sel[c].median())\n",
    "    # Fill categorical with mode\n",
    "    cat_cols = df_sel.select_dtypes(include='object').columns.tolist()\n",
    "    for c in cat_cols:\n",
    "        df_sel[c] = df_sel[c].fillna(df_sel[c].mode().iloc[0] if not df_sel[c].mode().empty else '')\n",
    "    return df_sel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b577dd",
   "metadata": {
    "id": "e8b577dd"
   },
   "source": [
    "## Step 4 — Conservative Outlier Handling\n",
    "Remove extreme outliers using 1st/99th percentiles and 3×IQR to preserve astrophysical extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749ac091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.184181Z",
     "iopub.status.busy": "2025-10-05T21:52:52.183136Z",
     "iopub.status.idle": "2025-10-05T21:52:52.196893Z",
     "shell.execute_reply": "2025-10-05T21:52:52.194225Z"
    },
    "id": "749ac091"
   },
   "outputs": [],
   "source": [
    "def remove_extreme_outliers(df, numeric_cols, lower_q=0.01, upper_q=0.99, multiplier=3.0):\n",
    "    q_low = df[numeric_cols].quantile(lower_q)\n",
    "    q_high = df[numeric_cols].quantile(upper_q)\n",
    "    iqr = q_high - q_low\n",
    "    lower = q_low - multiplier * iqr\n",
    "    upper = q_high + multiplier * iqr\n",
    "    mask = ~((df[numeric_cols] < lower) | (df[numeric_cols] > upper)).any(axis=1)\n",
    "    return df[mask].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0c954",
   "metadata": {
    "id": "efb0c954"
   },
   "source": [
    "## Step 5 — Build Preprocessor (Scaler + Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8d04f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.206057Z",
     "iopub.status.busy": "2025-10-05T21:52:52.205146Z",
     "iopub.status.idle": "2025-10-05T21:52:52.217818Z",
     "shell.execute_reply": "2025-10-05T21:52:52.215116Z"
    },
    "id": "4f8d04f8"
   },
   "outputs": [],
   "source": [
    "def build_preprocessor(numeric_features, categorical_features):\n",
    "    num_pipe = Pipeline([('scaler', StandardScaler())])\n",
    "    cat_pipe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    preprocessor = ColumnTransformer([('num', num_pipe, numeric_features),\n",
    "                                      ('cat', cat_pipe, categorical_features)],\n",
    "                                     remainder='drop')\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c93c4d",
   "metadata": {
    "id": "d3c93c4d"
   },
   "source": [
    "## Step 6 — Training, Evaluation & Plotting Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e60130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.225699Z",
     "iopub.status.busy": "2025-10-05T21:52:52.224427Z",
     "iopub.status.idle": "2025-10-05T21:52:52.240647Z",
     "shell.execute_reply": "2025-10-05T21:52:52.238207Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train, preprocessor, save_dir=BASE_MODEL_DIR):\n",
    "    \"\"\"\n",
    "    Train multiple ML models and save them as pipelines.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        preprocessor: Fitted preprocessor\n",
    "        save_dir: Directory to save models\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of trained model pipelines\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        'LogisticRegression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'XGBoost': XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, clf in models.items():\n",
    "        print(f'Training {name}...')\n",
    "        \n",
    "        # Create pipeline with preprocessor and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_path = os.path.join(save_dir, f'{name}_pipeline.pkl')\n",
    "        joblib.dump(pipeline, model_path)\n",
    "        print(f'Saved {name} to {model_path}')\n",
    "        \n",
    "        # Store in dictionary\n",
    "        trained_models[name] = pipeline\n",
    "    \n",
    "    return trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40e33bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.250899Z",
     "iopub.status.busy": "2025-10-05T21:52:52.249549Z",
     "iopub.status.idle": "2025-10-05T21:52:52.278459Z",
     "shell.execute_reply": "2025-10-05T21:52:52.275287Z"
    },
    "id": "e40e33bc"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save(models_dict, X_test, y_test, dataset_name, plots_dir=PLOTS_DIR, results_dir=RESULTS_DIR):\n",
    "    results = {'meta': {'dataset': dataset_name, 'run_id': str(uuid.uuid4())}, 'results': {}}\n",
    "    for name, model in models_dict.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') and len(model.classes_) == 2 else None # Only get proba for binary\n",
    "        metrics = {\n",
    "            'accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "            'precision': float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),\n",
    "            'recall': float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),\n",
    "            'f1': float(f1_score(y_test, y_pred, average='weighted', zero_division=0)),\n",
    "            'auc': float(roc_auc_score(y_test, y_proba)) if y_proba is not None else None\n",
    "        }\n",
    "        results['results'][name] = metrics\n",
    "\n",
    "        # confusion matrix plot\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(4,3))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
    "        plt.title(f'{dataset_name} - {name} Confusion Matrix')\n",
    "        cm_path = os.path.join(plots_dir, f'{dataset_name}_{name}_confusion.png')\n",
    "        plt.savefig(cm_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # ROC\n",
    "        if y_proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            plt.figure(figsize=(5,4))\n",
    "            plt.plot(fpr, tpr, label=f'AUC={metrics[\"auc\"]:.3f}')\n",
    "            plt.plot([0,1],[0,1],'--')\n",
    "            plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'{dataset_name} - {name} ROC')\n",
    "            plt.legend(loc='lower right')\n",
    "            roc_path = os.path.join(plots_dir, f'{dataset_name}_{name}_roc.png')\n",
    "            plt.savefig(roc_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    # save metrics json\n",
    "    out_path = os.path.join(results_dir, f'{dataset_name}_metrics.json')\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f'Saved metrics to {out_path}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00be1c3",
   "metadata": {
    "id": "c00be1c3"
   },
   "source": [
    "## Step 7 — Feature Importances & Save Top-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c8fbe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.289651Z",
     "iopub.status.busy": "2025-10-05T21:52:52.288377Z",
     "iopub.status.idle": "2025-10-05T21:52:52.313051Z",
     "shell.execute_reply": "2025-10-05T21:52:52.310200Z"
    },
    "id": "d7c8fbe4"
   },
   "outputs": [],
   "source": [
    "def extract_and_save_top_features(models_dict, feature_names, dataset_name, top_k=5, results_dir=RESULTS_DIR):\n",
    "    all_top = {}\n",
    "    for name, model in models_dict.items():\n",
    "        clf = model.named_steps['clf']\n",
    "        imp = None\n",
    "        if hasattr(clf, 'feature_importances_'):\n",
    "            imp = np.array(clf.feature_importances_)\n",
    "        elif hasattr(clf, 'coef_'):\n",
    "            # For linear models, coef_ can be 2D for multi-class, need to flatten\n",
    "            imp = np.abs(np.array(clf.coef_).ravel())\n",
    "        else:\n",
    "            print(f'No importances for {name}')\n",
    "            continue\n",
    "\n",
    "        # Get preprocessor output feature names after fitting\n",
    "        try:\n",
    "            pre = model.named_steps['preprocessor']\n",
    "            processed_names = pre.get_feature_names_out()\n",
    "        except Exception as e:\n",
    "            print(f'Error getting feature names out for {name}: {e}')\n",
    "            # Fallback: use provided feature_names (numeric + categorical before transform)\n",
    "            processed_names = feature_names\n",
    "\n",
    "        if len(imp) != len(processed_names):\n",
    "            print(f'Warning: importance len {len(imp)} != feature len {len(processed_names)} for {name}')\n",
    "            # If lengths still don't match, something is fundamentally wrong or\n",
    "            # the model type isn't handled correctly. Skip saving top features for this model.\n",
    "            continue\n",
    "\n",
    "        s = pd.Series(imp, index=processed_names).sort_values(ascending=False)\n",
    "        top = s.head(top_k).index.tolist()\n",
    "        all_top[name] = top\n",
    "        with open(os.path.join(results_dir, f'{dataset_name}_{name}_top_features.json'), 'w') as f:\n",
    "            json.dump(top, f, indent=2)\n",
    "\n",
    "        plt.figure(figsize=(6, max(2, len(top)*0.5)))\n",
    "        sns.barplot(x=s.head(top_k).values, y=s.head(top_k).index)\n",
    "        plt.title(f'{name} Top {top_k} features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f'{dataset_name}_{name}_topk.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'Saved top{top_k} for {name}')\n",
    "    return all_top\n",
    "\n",
    "def save_feature_medians(df, dataset_name, path=RESULTS_DIR):\n",
    "    med = df.median(numeric_only=True).to_dict()\n",
    "    with open(os.path.join(path, f'{dataset_name}_feature_medians.json'), 'w') as f:\n",
    "        json.dump(med, f, indent=2)\n",
    "    print('Saved medians for dataset:', dataset_name)\n",
    "    return med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206a138",
   "metadata": {
    "id": "7206a138"
   },
   "source": [
    "## Step 8 — Prediction Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e184c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.322960Z",
     "iopub.status.busy": "2025-10-05T21:52:52.322044Z",
     "iopub.status.idle": "2025-10-05T21:52:52.346338Z",
     "shell.execute_reply": "2025-10-05T21:52:52.343137Z"
    },
    "id": "a0e184c5"
   },
   "outputs": [],
   "source": [
    "def predict_full_input(model_name, dataset_name, input_full):\n",
    "    model_path = os.path.join(BASE_MODEL_DIR, f'{model_name}_pipeline.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "    X = pd.DataFrame([input_full], columns=model.named_steps['preprocessor'].feature_names_in_)\n",
    "    pred = int(model.predict(X)[0])\n",
    "    proba = float(model.predict_proba(X)[0][1]) if hasattr(model, 'predict_proba') else None\n",
    "    return {'prediction': pred, 'probability': proba}\n",
    "\n",
    "def predict_from_top5(model_name, dataset_name, top5_dict):\n",
    "    med_path = os.path.join(RESULTS_DIR, f'{dataset_name}_feature_medians.json')\n",
    "    if not os.path.exists(med_path):\n",
    "        raise FileNotFoundError('Medians not found. Run training to save medians first.')\n",
    "    with open(med_path,'r') as f:\n",
    "        medians = json.load(f)\n",
    "    top_path = os.path.join(RESULTS_DIR, f'{dataset_name}_{model_name}_top_features.json')\n",
    "    if not os.path.exists(top_path):\n",
    "        raise FileNotFoundError('Top features file not found. Run extract_and_save_top_features first.')\n",
    "    with open(top_path,'r') as f:\n",
    "        top_feats = json.load(f)\n",
    "    cols_path = os.path.join(RESULTS_DIR, f'{dataset_name}_training_columns.json')\n",
    "    if not os.path.exists(cols_path):\n",
    "        raise FileNotFoundError('Training columns file not found. Run training step that saves training columns.')\n",
    "    with open(cols_path,'r') as f:\n",
    "        training_cols = json.load(f)\n",
    "    row = {}\n",
    "    for c in training_cols:\n",
    "        if c in medians:\n",
    "            row[c] = medians[c]\n",
    "        else:\n",
    "            row[c] = 0\n",
    "    # overwrite with provided top5\n",
    "    for k,v in top5_dict.items():\n",
    "        if k not in row:\n",
    "            raise ValueError(f'Feature {k} not in training columns')\n",
    "        row[k] = v\n",
    "    df_row = pd.DataFrame([row], columns=training_cols)\n",
    "    model_path = os.path.join(BASE_MODEL_DIR, f'{model_name}_pipeline.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "    pred = model.predict(df_row)[0]\n",
    "    proba = model.predict_proba(df_row)[0][1] if hasattr(model, 'predict_proba') else None\n",
    "    return {'prediction': int(pred), 'probability': float(proba) if proba is not None else None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165cd35",
   "metadata": {
    "id": "5165cd35"
   },
   "source": [
    "## Step 9 — Example: Full workflow runner\n",
    "Call `run_full_workflow('K2')` to execute everything for the K2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ee7395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.357028Z",
     "iopub.status.busy": "2025-10-05T21:52:52.356118Z",
     "iopub.status.idle": "2025-10-05T21:52:52.377264Z",
     "shell.execute_reply": "2025-10-05T21:52:52.374172Z"
    },
    "id": "41ee7395"
   },
   "outputs": [],
   "source": [
    "def run_full_workflow(dataset_name='K2'):\n",
    "    path = get_dataset_path(dataset_name)\n",
    "    if path is None:\n",
    "        raise FileNotFoundError(f'Dataset mapping for {dataset_name} not found. Put K2.csv under ../../Data Sources/ or use local_k2 mapping.')\n",
    "    df0 = load_raw_dataset(path)\n",
    "    df1 = preprocess_initial(df0)\n",
    "    df_sel = select_and_prepare_k2(df1)\n",
    "    print('After selection:', df_sel.shape)\n",
    "    numeric_cols = df_sel.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in ['Target']]\n",
    "    df_clean = remove_extreme_outliers(df_sel, numeric_cols, lower_q=0.01, upper_q=0.99, multiplier=3.0)\n",
    "    print('After outlier removal:', df_clean.shape)\n",
    "    df_clean = df_clean.dropna(subset=['Target'])\n",
    "    training_cols = [c for c in df_clean.columns if c != 'Target']\n",
    "    with open(os.path.join(RESULTS_DIR, f'{dataset_name}_training_columns.json'), 'w') as f:\n",
    "        json.dump(training_cols, f, indent=2)\n",
    "    save_feature_medians(df_clean[training_cols], dataset_name)\n",
    "    X = df_clean[training_cols]\n",
    "    y = df_clean['Target']\n",
    "    numeric_features = X.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    preprocessor = build_preprocessor(numeric_features, categorical_features)\n",
    "    if len(X) < 10:\n",
    "        raise ValueError('Not enough data after cleaning to train models.')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(y.unique())>1 else None)\n",
    "    trained = train_models(X_train, y_train, preprocessor, save_dir=BASE_MODEL_DIR)\n",
    "    res = evaluate_and_save(trained, X_test, y_test, dataset_name)\n",
    "    extract_and_save_top_features(trained, numeric_features + categorical_features, dataset_name, top_k=5, results_dir=RESULTS_DIR)\n",
    "    print('Workflow done for', dataset_name)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lbb5WfEycAZ1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "execution": {
     "iopub.execute_input": "2025-10-05T21:52:52.388476Z",
     "iopub.status.busy": "2025-10-05T21:52:52.386944Z",
     "iopub.status.idle": "2025-10-05T21:52:57.265657Z",
     "shell.execute_reply": "2025-10-05T21:52:57.262088Z"
    },
    "id": "lbb5WfEycAZ1",
    "outputId": "f4aa9cab-109e-4d03-85c6-1fb8b9028ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data Sources\\k2.csv -> shape: (4004, 129)\n",
      "After selection: (3982, 14)\n",
      "After outlier removal: (3943, 14)\n",
      "Saved medians for dataset: local_k2\n",
      "Training RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RandomForest to ../static/models\\RandomForest_pipeline.pkl\n",
      "Training LogisticRegression...\n",
      "Saved LogisticRegression to ../static/models\\LogisticRegression_pipeline.pkl\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost to ../static/models\\XGBoost_pipeline.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to ../static/results\\local_k2_metrics.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top5 for RandomForest\n",
      "Warning: importance len 4779 != feature len 1593 for LogisticRegression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top5 for XGBoost\n",
      "Workflow done for local_k2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'meta': {'dataset': 'local_k2',\n",
       "  'run_id': '2f1af870-a6f1-428c-861f-331992445424'},\n",
       " 'results': {'RandomForest': {'accuracy': 1.0,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0,\n",
       "   'auc': None},\n",
       "  'LogisticRegression': {'accuracy': 1.0,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0,\n",
       "   'auc': None},\n",
       "  'XGBoost': {'accuracy': 1.0,\n",
       "   'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0,\n",
       "   'auc': None}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = run_full_workflow('local_k2')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e7957",
   "metadata": {
    "id": "4d7e7957"
   },
   "source": [
    "## Step 10 — Final Notes\n",
    "After running the workflow, Flask can serve JSON and PNG artifacts from `../static/results` and `../static/plots`. Use the prediction helpers to accept top-5 feature inputs from the UI."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
